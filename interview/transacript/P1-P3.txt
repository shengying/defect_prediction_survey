P1 (R1-P3) DEV = 8

P2 (R1-P2) TESTING= 6
P3 (R1-P1) PM = 7

到底是辅助工具，还是完全替代人工。

团队规模： P3 12 | P1 30 

工作职责： Team Leader

参与项目：　
P1: Java (遗留系统，维护和功能升级), .NET (C# VB，遗留系统，功能升级)
P3: 脚本 

历史数据可用性：
JIRA来记录Bug数据，SCRUM流程；

购买决策流程：
DEV/PM需求驱动 > 项目组沟通 > 报告上级，申请经费 > Account Manager上级批准 > 统一采购，比价

Factor：
P1： 需求 驱动 成本
P2: 性价比
会考虑开源。

工具推广：
自需求出发，自下而上。

广告：
不会点击。

同事推荐：
P1： 会考虑
P3： 会考虑

同事推荐 示例和细节：
比较关系好的、信任感强的，会比较有用。

需求驱动：
工具对我们的帮助，就会试一下，再做定夺。

测试时间占比：
质量要求比较高的项目，测试的时间会上升；甚至可以达到80%。

代码审查：
对比较复杂的功能，着重花时间来code review。

期待输入：
代码的复杂度，条件语句过多，大括号

测试准备工作:
过需求，设计测试用例，部署测试环境，熟悉功能。
占30%的时间。

Factor工作量分配：
客户的需求，从业务出发，常用、重要的功能会优先级高；
DEV的修改从技术上的影响面，和工作量有关。
Test Case的coverage；
理想的coverage是100%。

测试/code review结束标准：
不同级别的bug有可以接受的存在量；

Factor:
可靠性，可以相信工具的结果。
全自动，不用人工干预，替换人工code review。
误报率30%-40%。
误报率20%-30%。
精确度： 设计引入的Bug本身就很不容易看出来，所以要求不高；非设计引入、人为编码引入的Bug需要有很高的精确度。

Factor排序：
P2: 相对优势（有优势才会用） > 兼容性（用得上） > 可试用性（用起来简单） > 复杂度 > 可重用性
P3: 相对优势 > 兼容性（开发语言等） > 可试用性 > 复杂度 > 可重用性
P1: 可试用性 > 可重创性 > 相对优势 > 复杂度 > 兼容性
