1/ P8： （R3-P3） 7 = PM （投入产出比。到底defect prediction是一个锦上添花，还是雪中送炭。使用的场景。）

2/ P9： （R3-P2） 2 = DEV （感知和认知。）

3/ P10： (R3-P1) TEST = 5

Factor: 
4/ 1） 是否有坚强、本质性的理论，作为背景。
5/ 2） 市场的刚需、痛点；业界被认知的程度；

6/ 是否纳入一个实践体系；
7/ 江湖地位。
3） 8/ 缺陷预测的有没有很强的需求，能解决什么问题？业务本身的认可：不同的项目，对质量的要求不同，对质量要求不高的系统，缺陷预测有什么作用呢？
9/ 4） 业界大佬背书。
10/ 5） 应用场景太小。

11/ （需求变更） 希望功能： 需求变更产生的bug，原本的代码不适用，产生了bug，就是因为需求的变更。


12/ 我的问题： code smell和defect prediction之间的差别

13/ 和代码分析工具的差异；

14/ （代码以外，程序员行为会产生bug） 建议输入：缺陷预测基于行为：程序员输入一个代码，或者comments，或者commit的时候输入message，都有可能引入问题；
15/ （QA行为也可能会引入Bug） QA的很多行为，可能会引入bug。

16/ 期待结果：code的缺陷，不仅仅应该是代码的因素。

17/ 期待结果：预测各个阶段DEV，QA，部署的时候，可能产生的问题。

18/ 期待结果：包含coding层面的bug，功能层面的bug，configuration的bug

19/ 算法： 能否知道系统如何运作；

20/ 算法：仅仅基于历史数据，concern数据不够，算法本身是否去分析之前bug产生的原因，还是只是通过现象来倒推；

21/ 算法：如何来考虑历史与现在之间的迁移；

22/ 算法：是否通过反馈更新模型。

23/ 准备工作: 模型需要比较成熟，没有冷启动。


24/ 广告： 3人都没有作用

25/ Peer： 和同事更好的聊一聊

26/ （为什么会使用？） 需要口碑，需要真实的场景，

27/ 有缺陷预测的需求；

28/ 需要评估一下；

29/ 上级： QA表示 - PM和架构师是驱动QA使用新工具的关键； 其他两位不重要。

30/ 需求驱动： 没有需求，没有场景；

31/ 使用时机：不设置具体的使用时间点，而是把工具融入整个软件工程的流程中；

32/ 融入集成开发环境中，透明的产生效果，不是作为一个单独的工具产生；

33/ 面向business推这个概念。

性能评估： （iPhone pm2 34：51） 
34/ （benchmark，说明数据和准确率的关系）识别需要的数据量和准确率的关系。达到一个标准后，才考虑推广。

性能评估： 
35/ 1000个文件，只能推荐100个需要我检查的，10%的effort；
36/ 10万行，1000个可能的误报，5%的误报。

37/ 性能评估： 项目管理层面，尝试工具的effort大于它的回报，就不会使用。

38/ 不能给组里带来额外的负担。

建议输入： 

39/ 代码开发人员的可信程度，是决定code review工作量的因素；
40/ 如何去衡量、量化一个代码开发人员的可信程度(trust)，以及对这个模块的熟悉程度。

41/ 准备工作: 数据收集很花时间，别人的项目的数据的个性化不强，需要准确地衡量这些数据。

42/ 性能评估： （iPhone pm2 43:11） 客观性。感觉没有code analysis的结果客观。

43/ 性能评估： 在短时间内，提高code review的效率。code analysis工具可以得到。

44/ Awareness： 
是在没有时间/cost限制的时候，defect prediction可是对质量带来更高提升的。

45/ 期待结果： 感知和认知，defect prediction是可以做到更好的感知，但是无法自我认知，本身AI的限制。

46/ 使用时机： 利用defect prediction去报bug，数量可以得到保证；然后对QA的结果进行验证，QA reviewer,从而达到QA本身的目标。test case和业务进行关联。

47/ Factor: 投入产出比。到底defect prediction是一个锦上添花，还是雪中送炭。

48/ 使用时机： 对产品而非项目有效。打磨产品，让其质量不断提升；项目则是有明确的accept criteria衡量，必须完成所有的test case才可以。

49/ 期待结果： 软件工程各个流程各个维度的缺陷，而不仅仅是代码维度的缺陷。

Factor: 
50/ R3-P3: 相对优势 > 兼容性(能不能用) > 可试用性 > 可重创性 > 复杂度
51/ Level 1: 性能、成本和潜在收益
52/ Level 2: 希望的功能 - 项目用的上（有合适的应用场景），给足够多的信息，解释越精确越好，辅助解决问题的信息越多越好；其他信息越少越好。
Level 3: 其他

53/ R3-P2: 兼容性（能不能用） > 重创性 （能不能灵活用） > 可试用性 （尝试） > 复杂度 > 相对优势(主要是合适，利用自己的评价指标，是最好的) 
54/ Level 1：（强需求） 基础功能、可定制化、成本和潜在收益
55/ Level 2:(使用规范) 
56/ Level 3：（易用性）
57/ Level 4: （扩展和生态） Suite，社区，别人在不在用
58/ Level 5： 其他

59/ R3-P1: 相对优势（受权威人士的推荐和强制使用） > 可试用性 > 兼容性 > 可重创性 > 复杂度
60/ Level 1： 投入产出比，成本和潜在收益（阿里云有一个免费的自动化测试，4小时出结果）
61/ Level 2: 性能、成本、希望具备的功能
62/ Level 3: 原因和可靠性相关，如果之前的性能好，那么原因不重要